## 메모리 관리 실습
debug모드를 켜면 LLM이 어떤식으로 동작되고 있는지 상세히 알아볼 수 있다.
```py
import langchain
langchain.debug = True
```

## 실제로 메모리가 어떻게 동작하는지 알아보자
* 메시지와 trimmer 정의
```py
messages = [
    SystemMessage(content="사용자의 질문에 2문장 이내로 짧게 대답해"),
    HumanMessage(content="오늘은 피자를 먹어야지!"),
    AIMessage(content="정말 좋은 생각이야. 음료는 무엇으로 할 거야?"),
    HumanMessage(content="내일은 수영을 가야지!"),
    AIMessage(content="수영이라니, 정말 좋은 운동이야. 수영장은 어디로 다녀?"),
    HumanMessage(content="주말에는 영화를 보러 갈 거야!"),
    AIMessage(content="주말이 벌써부터 기다려지겠는걸? 보려고 생각해둔 영화가 있어?"),
]

trimmer = trim_messages(
    max_tokens=120,
    token_counter=llm,
    strategy="last",
    include_system=True,
    start_on=HumanMessage,
)
```
max_tokens: 토큰 120개만 남기고 자른다.  
strategy: 최근 대화를 남기고 과거대화를 지운다.  
include_system: 시스템메시지 남긴다.
start_on: HumanMessage부터 시작한다.

```py
trimmer.invoke(messages)


=> [SystemMessage(content='사용자의 질문에 2문장 이내로 짧게 대답해', additional_kwargs={}, response_metadata={}),
 HumanMessage(content='내일은 수영을 가야지!', additional_kwargs={}, response_metadata={}),
 AIMessage(content='수영이라니, 정말 좋은 운동이야. 수영장은 어디로 다녀?', additional_kwargs={}, response_metadata={}),
 HumanMessage(content='주말에는 영화를 보러 갈 거야!', additional_kwargs={}, response_metadata={}),
 AIMessage(content='주말이 벌써부터 기다려지겠는걸? 보려고 생각해둔 영화가 있어?', additional_kwargs={}, response_metadata={})]
```  

* parser 정의
```py
from langchain_core.output_parsers import StrOutputParser
parser = StrOutputParser()
chain = trimmer | llm | parser
```

* debug모드로 실행
```py
chain.invoke(messages + [
    HumanMessage(content="오늘 뭘 먹는다고 했지?")
    ]
)

```

=> trimmer에 의해 메시지가 오늘 뭘 먹는지에 대한 내용이 삭제되었으므로 llm은 이를 인지하지 못한다.
```
[chain/start] [chain:RunnableSequence] Entering Chain run with input:
[inputs]
[chain/start] [chain:RunnableSequence > chain:trim_messages] Entering Chain run with input:
[inputs]
[chain/end] [chain:RunnableSequence > chain:trim_messages] [3ms] Exiting Chain run with output:
[outputs]
[llm/start] [chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:
{
  "prompts": [
    "System: 사용자의 질문에 2문장 이내로 짧게 대답해\nHuman: 내일은 수영을 가야지!\nAI: 수영이라니, 정말 좋은 운동이야. 수영장은 어디로 다녀?\nHuman: 주말에는 영화를 보러 갈 거야!\nAI: 주말이 벌써부터 기다려지겠는걸? 보려고 생각해둔 영화가 있어?\nHuman: 오늘 뭘 먹는다고 했지?"
  ]
}
[llm/end] [chain:RunnableSequence > llm:ChatOpenAI] [1.43s] Exiting LLM run with output:
{
  "generations": [
    [
      {
        "text": "아직 정하지 않았어? 맛있는 음식을 선택하길 바랄게!",
        "generation_info": {
          "finish_reason": "stop",
          "logprobs": null
        },
        "type": "ChatGeneration",
        "message": {
          "lc": 1,
...
[chain/end] [chain:RunnableSequence] [1.44s] Exiting Chain run with output:
{
  "output": "아직 정하지 않았어? 맛있는 음식을 선택하길 바랄게!"
}
```

```py
chain.invoke(messages + [
    HumanMessage(content="주말에는 내가 뭘 한다고 했지?")
    ]
)
```

=> 주말에는 내가 영화를 보러간다는 내용이 삭제되지 않았으므로 llm은 답변할 수 있다.
```
[chain/start] [chain:RunnableSequence] Entering Chain run with input:
[inputs]
[chain/start] [chain:RunnableSequence > chain:trim_messages] Entering Chain run with input:
[inputs]
[chain/end] [chain:RunnableSequence > chain:trim_messages] [5ms] Exiting Chain run with output:
[outputs]
[llm/start] [chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:
{
  "prompts": [
    "System: 사용자의 질문에 2문장 이내로 짧게 대답해\nHuman: 주말에는 영화를 보러 갈 거야!\nAI: 주말이 벌써부터 기다려지겠는걸? 보려고 생각해둔 영화가 있어?\nHuman: 주말에는 내가 뭘 한다고 했지?"
  ]
}
[llm/end] [chain:RunnableSequence > llm:ChatOpenAI] [1.18s] Exiting LLM run with output:
{
  "generations": [
    [
      {
        "text": "주말에는 영화를 보러 간다고 했어! 어떤 영화 볼 예정이야?",
        "generation_info": {
          "finish_reason": "stop",
          "logprobs": null
        },
        "type": "ChatGeneration",
        "message": {
          "lc": 1,
...
[chain/end] [chain:RunnableSequence] [1.19s] Exiting Chain run with output:
{
  "output": "주말에는 영화를 보러 간다고 했어! 어떤 영화 볼 예정이야?"
}
```